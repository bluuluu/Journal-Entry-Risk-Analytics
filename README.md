# Journal Entry Risk Analytics

Audit-analytics style project to surface risky journal entries from general ledger data. The pipeline is SQL-first, with a Python layer for orchestration and risk scoring, plus an outline for a dashboard auditors can use to drill into the riskiest entries instead of broad sampling.

## Repository structure
- `data/gl_sample.csv` — sample general-ledger data to develop against.
- `data/risk_scores.csv` — example output generated by the Python scorer.
- `sql/00_schema.sql` — Postgres schema for GL and calendar metadata.
- `sql/01_feature_flags.sql` — SQL feature engineering and risk scoring query.
- `src/risk_scoring.py` — pandas-based scorer mirroring the SQL logic.
- `notebooks/` — placeholder for exploratory analysis or dashboard prototyping.

## What this delivers
- Per-entry risk score with transparent flags (after-hours, round-dollar, rare account pairings, magnitude outliers, weekend/period-close, approval pending, keyword hits, user volume spikes).
- A SQL-first approach so logic is explainable and auditable.
- A Python entry point to run locally or schedule (Airflow/Prefect/cron).
- Dashboard-ready data for Tableau/Power BI showing top risky entries, flag breakdowns, and drill-through to line-level detail.

## Quickstart (Python path)
1) Install dependencies (pandas, numpy):
```bash
python3 -m venv .venv && source .venv/bin/activate
pip install pandas numpy
```
2) Run the scorer on the sample data:
```bash
python3 src/risk_scoring.py --input data/gl_sample.csv --output data/risk_scores.csv
```
3) Explore the output in `data/risk_scores.csv` (ready for a dashboard ingest).

## Quickstart (SQL path in Postgres)
1) Run schema setup and load the sample data:
```sql
\i sql/00_schema.sql;
\COPY audit_analytics.gl_entries FROM 'data/gl_sample.csv' WITH CSV HEADER;
```
2) Generate risk-scored entries:
```sql
\i sql/01_feature_flags.sql;
```
3) Point Tableau/Power BI to `audit_analytics` and start with a table sorted by `risk_score` plus filters for entity, account, user, period, and flags.

## How the scoring works (tunable)
- Amount magnitude: per-account z-score for outliers.
- Round-dollar detection: `abs(amount) % 100 = 0`.
- Timing: after-hours relative to entity business hours; weekend; period-close.
- User behavior: monthly posting volume z-score per user.
- Account pair rarity: rare account/offset combinations within each account.
- Description cues: simple keyword hits (`gift`, `manual`, etc.).
- Approval state: pending/unapproved entries carry risk weight.
- Weights are set in `sql/01_feature_flags.sql` and `src/risk_scoring.py` and are capped at 100; adjust to align with firm policy.

## Dashboard outline
- Top N risky entries table with per-flag breakdown and drill-through to JE line detail.
- Filters: entity, account, user, date range, source, approval status, flag toggles.
- Visuals: time-of-day heatmap for postings, weekend vs weekday split, risk score distribution, bar of top recurring users/entities by aggregate risk.
- Drill path: Entry -> Lines -> Source info (user, timestamp, approval) -> Flags triggered.

## What it does
- What it does: flags risky journal entries so auditors focus on likely issues instead of broad sampling, using explainable indicators (timing, amount patterns, approvals, user behavior, keywords).
- How it works: SQL-first feature engineering (`sql/01_feature_flags.sql`) with a matching pandas pipeline (`src/risk_scoring.py`) that scores sample GL data (`data/gl_sample.csv`) into a dashboard-ready file (`data/risk_scores.csv`).
- Output: per-entry risk_score plus individual flag columns ready for Power BI/Tableau; dashboards highlight the riskiest entries with flag breakdowns and drill-through to line detail.
- Why it matters: improves audit efficiency and coverage; logic is transparent, tunable, and can be back-tested; designed to run locally or on a scheduler (Airflow/Prefect/cron).
- Next steps you can mention: per-entity business-hour configs, data-quality checks, scheduled runs with logging, row-level security in BI, and calibration against historical findings.

## Next steps to productionize
- Parameterize business hours/weekends per entity from a config table.
- Add data-quality checks (duplicate JEs, missing timestamps, invalid time zones).
- Automate runs via scheduler (Airflow/Prefect/cron) with logging and run metadata.
- Add row-level security in BI tool by entity and workpaper references for audit evidence.
- Back-test against historical issues to calibrate weights and thresholds.
